{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virus Gene Extraction, FASTA File Creation and Salmon Reference File Creation Workflow\n",
    "This workflow extracts viral genes from various sources, saves them in FASTA format, and generates metadata for custom viruses. It also creates a \"gentrome\" file that includes human and viral transcriptomes for downstream RNA-seq quantification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a main directory (DATA_DIR) with according sub directories. </br>\n",
    "DATA_DIR/general_parts/ </br>\n",
    "DATA_DIR/plots_with_decoy/ </br>\n",
    "DATA_DIR/scripts/ </br>\n",
    "DATA_DIR/human_genom/ </br>\n",
    "DATA_DIR/viral_genes </br>\n",
    " </br>\n",
    "These than also need certain files in them. </br>\n",
    "\n",
    "General Data: </br>\n",
    "    internal-24q4_v84-omicsdefaultmodelprofiles.csv (from taiga) </br>\n",
    "    pr_table_subset.csv (some mapping file with the mapped ids (ProfileID, ModelID, SequenceID), from Gumbo) </br>\n",
    " </br>\n",
    "Viral Genes: </br>\n",
    "    viral_gene_identifiers.csv (get from GDC, https://gdc.cancer.gov/about-data/gdc-data-processing/gdc-reference-files, https://gdc.cancer.gov/system/files/public/file/GRCh83.d1.vd1_virus_decoy.txt) </br>\n",
    "    DATA_DIR/viral_genes/{virus_name}/{virus_name}_raw.txt (any other virus genes that are not in the GDC list but still shall be included in the reference file later) </br>\n",
    "    DATA_DIR/viral_genes/{virus_name}/pLenti-raw-genes.txt (any Lenti virus genes that shall be included in the reference file later) </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have installed all packages and run this command in your terminal: </br>\n",
    "*pip install requests pandas biopython*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import requests\n",
    "import zipfile\n",
    "import csv\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import re\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = \"/Users/ptrollma/experiment_data/virus_genes/\" # adjust this to your directory path\n",
    "GENERAL_DATA = DATA_DIR + \"general_parts/\"\n",
    "META_DATA_DIR = DATA_DIR + \"viral_genes/\"\n",
    "PLOT_DIR = DATA_DIR + \"plots_with_decoy/\"\n",
    "HUMAN_GENE_DIR = DATA_DIR + \"human_genom/\"\n",
    "SCRIPTS_DIR = DATA_DIR + \"scripts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if directories exist\n",
    "directories = {\n",
    "    \"META_DATA_DIR\": DATA_DIR + \"viral_genes/\",\n",
    "    \"PLOT_DIR\": DATA_DIR + \"plots_with_decoy/\",\n",
    "    \"HUMAN_GENE_DIR\": DATA_DIR + \"human_genom/\",\n",
    "    \"GENERAL_DATA\": DATA_DIR + \"general_parts/\",\n",
    "    \"SCRIPTS_DIR\": DATA_DIR + \"scripts/\"\n",
    "}\n",
    "\n",
    "# Check if DATA_DIR exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    # Iterate over each directory and create it if it doesn't exist\n",
    "    for dir_name, dir_path in directories.items():\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "            print(f\"{dir_name} created: {dir_path}\")\n",
    "else:\n",
    "    print(f\"{DATA_DIR} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to list all bam files in the google bucket\n",
    "You need the name and permission of the google bucket were the bam files are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"gs://cclebams/rnasq_hg38/\"   # Define your bucket name\n",
    "user = 'philipp-trollmann'                  # Add a user that has permission for this bucket\n",
    "\n",
    "# Run gsutil command to list all BAM files and save to CSV\n",
    "with open(f\"{SCRIPTS_DIR}bam_file_names.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"hg38_rna_bam\"])  # Optional header\n",
    "    result = subprocess.run(\n",
    "            f\"gsutil -u {user} ls {bucket_name}*Aligned.sortedByCoord.out.bam\", \n",
    "            shell=True, capture_output=True, text=True, check=True\n",
    "    )\n",
    "    for file_path in result.stdout.splitlines():\n",
    "        writer.writerow([file_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to write column with ModelID and bam file location (only get the default condition measurement)\n",
    "Make a file that has all the model IDs (cell lines) and the according bam file locations in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_files_df = pd.read_csv(f\"{SCRIPTS_DIR}bam_file_names.csv\")\n",
    "profile_id = pd.read_csv(f\"{GENERAL_DATA}internal-24q4_v84-omicsdefaultmodelprofiles.csv\")\n",
    "id_map = pd.read_csv(f\"{GENERAL_DATA}pr_table_subset.csv\")\n",
    "\n",
    "# Create a dictionary for mapping column names to IDs\n",
    "name_to_id = dict(zip(id_map['MainSequencingID'], id_map['ModelID']))\n",
    "\n",
    "# Use to only take the untreated profiles, and only take the main profile of each cell line\n",
    "profile_id_rna = profile_id[profile_id['ProfileType'] == 'rna']\n",
    "profile_id_rna_merge = pd.merge(profile_id_rna, id_map, on='ProfileID', how='left')\n",
    "\n",
    "# Get sequencing id from bucket path and map to ModelID, remove NaNs and duplicates\n",
    "modelid_bucket_df = pd.DataFrame(columns= [\"entity:sample_id\", \"hg38_rna_bam\", \"sequencing_id\"])\n",
    "modelid_bucket_df[\"hg38_rna_bam\"] = bam_files_df[\"hg38_rna_bam\"]\n",
    "modelid_bucket_df['sequencing_id'] = modelid_bucket_df['hg38_rna_bam'].str.extract(r'(CDS-\\w{6})')\n",
    "modelid_bucket_df[\"entity:sample_id\"] = modelid_bucket_df['sequencing_id'].map(name_to_id)\n",
    "modelid_bucket_df = modelid_bucket_df[modelid_bucket_df['sequencing_id'].isin(profile_id_rna_merge['MainSequencingID'])]\n",
    "\n",
    "modelid_bucket_df_sorted= modelid_bucket_df.sort_values(by='entity:sample_id').iloc[:,:2]\n",
    "modelid_bucket_df_sorted.to_csv(f\"{SCRIPTS_DIR}cell_line_google_bucket_index.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download human genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs to the files\n",
    "transcriptome_url = \"ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/gencode.v38.transcripts.fa.gz\"\n",
    "genome_url = \"ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_38/GRCh38.primary_assembly.genome.fa.gz\"\n",
    "\n",
    "# Set the destination directory for downloads\n",
    "download_dir = HUMAN_GENE_DIR\n",
    "download_dir.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Paths to save the downloaded files\n",
    "transcriptome_file = download_dir + \"gencode.v38.transcripts.fa.gz\"\n",
    "genome_file = download_dir + \"GRCh38.primary_assembly.genome.fa.gz\"\n",
    "\n",
    "# Download the transcriptome file\n",
    "urllib.request.urlretrieve(transcriptome_url, transcriptome_file)\n",
    "print(f\"Downloaded: {transcriptome_file}\")\n",
    "\n",
    "# Download the genome file\n",
    "urllib.request.urlretrieve(genome_url, genome_file)\n",
    "print(f\"Downloaded: {genome_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to download all wanted viral gene sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the viral gene identifiers from CSV\n",
    "viral_identifiers_df = pd.read_csv(META_DATA_DIR + \"viral_gene_identifiers.csv\")\n",
    "\n",
    "# List of specific viruses to download\n",
    "virus_downloads = ['CMV ( HHV-5)', 'EBV (HHV-4)', 'HBV', 'HCV-1', 'HCV-2', 'HIV-1', 'HIV-2', 'HHV-8, KSHV', \n",
    "                   'HTLV-1', 'MCV', 'SV40', 'HPV16', 'HPV18', 'HPV26', 'HPV53', 'HPV1', 'HPV2', 'HPV4', \n",
    "                   'HPV5', 'HPV6', 'HPV7', 'HPV9', 'HPV10', 'HPV24', 'HPV32', 'HPV34', 'HPV41', 'HPV48', \n",
    "                   'HPV49', 'HPV50', 'HPV54', 'HPV60', 'HPV61', 'HPV63', 'HPV88', 'HPV90', 'HPV92', \n",
    "                   'HPV96', 'HPV98', 'HPV99', 'HPV100', 'HPV101', 'HPV103', 'HPV104', 'HPV105', 'HPV108', \n",
    "                   'HPV109', 'HPV112', 'HPV113', 'HPV114', 'HPV115', 'HPV116', 'HPV121', 'HPV126', \n",
    "                   'HPV127', 'HPV128', 'HPV129', 'HPV131', 'HPV132', 'HPV134', 'HPV135', 'HPV136', \n",
    "                   'HPV137', 'HPV140', 'HPV144', 'HPV148', 'HPV154', 'HPV166', 'HPV167', 'HPV178', \n",
    "                   'HPV179']\n",
    "\n",
    "# Get a dictionary of identifiers from the CSV\n",
    "lookup_dict = dict(zip(viral_identifiers_df['Abbreviation'], viral_identifiers_df['RefSeq']))\n",
    "\n",
    "# Filter the dictionary to include only the viruses in the virus_downloads list\n",
    "result_dict = {key: lookup_dict[key] for key in virus_downloads if key in lookup_dict}\n",
    "\n",
    "# Create an empty list to store metadata\n",
    "metadata_list = []\n",
    "\n",
    "def extract_genomic_range_from_fna(file_path):\n",
    "    \"\"\"Extracts the genomic range from the first line of the gene.fna file.\"\"\"\n",
    "    with open(file_path, 'r') as fna_file:\n",
    "        first_line = fna_file.readline().strip()  # Read the first line\n",
    "        match = re.search(r':c?(\\d+)-(\\d+)', first_line)  # Regex to capture the range\n",
    "        if match:\n",
    "            start, end = match.groups()\n",
    "            return f\"{start}-{end}\"\n",
    "        else:\n",
    "            return None  # Handle case where range is not found\n",
    "\n",
    "def get_gene_ids_from_refseq(refseq_id, page_size=200):\n",
    "    \"\"\"Fetch gene IDs with pagination to manage large responses.\"\"\"\n",
    "    all_gene_info = []\n",
    "    page_number = 1  # Track the current page\n",
    "\n",
    "    while True:\n",
    "        # URL for the API request with a hypothetical pagination system\n",
    "        url = f\"https://api.ncbi.nlm.nih.gov/datasets/v2alpha/gene/accession/{refseq_id}\"\n",
    "        params = {\n",
    "            \"page_size\": page_size,  # Limit the number of results per request\n",
    "            \"page\": page_number      # Request the current page\n",
    "        }\n",
    "        headers = {\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract gene information from the response\n",
    "            gene_info = [\n",
    "                (\n",
    "                    report['gene']['gene_id'], \n",
    "                    report['gene']['symbol'],\n",
    "                    f\"{report['gene']['annotations'][0]['genomic_locations'][0]['genomic_range']['begin']}-\"\n",
    "                    f\"{report['gene']['annotations'][0]['genomic_locations'][0]['genomic_range']['end']}\"\n",
    "                )\n",
    "                for report in data['reports']\n",
    "            ]\n",
    "            \n",
    "            # Add current page's results to the total list\n",
    "            all_gene_info.extend(gene_info)\n",
    "\n",
    "            # Check if there's a \"next\" page or if we're done\n",
    "            if 'next_page' in data:\n",
    "                page_number += 1  # Move to the next page\n",
    "            else:\n",
    "                break  # No more pages, exit the loop\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}, {response.text}\")\n",
    "            break\n",
    "    \n",
    "    return all_gene_info\n",
    "\n",
    "def download_gene_data(gene_id, dir, gene_name, virus_name, genomic_range):\n",
    "    # Replace '/' in gene and virus names with '_'\n",
    "    sanitized_gene_name = gene_name.replace('/', '_')\n",
    "    sanitized_virus_name = virus_name.replace('/', '_')\n",
    "   \n",
    "    url = f\"https://api.ncbi.nlm.nih.gov/datasets/v2alpha/gene/id/{gene_id}/download\"    \n",
    "    params = {\n",
    "        \"include_annotation_type\": [\"FASTA_GENE\"],  # Request the gene FASTA file\n",
    "        \"filename\": f\"{sanitized_virus_name}_{sanitized_gene_name}_{gene_id}_datasets.zip\"\n",
    "    }\n",
    "    headers = {\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        zip_path = os.path.join(dir, f\"{sanitized_virus_name}_{sanitized_gene_name}_{gene_id}_datasets.zip\")\n",
    "        unzip_path = os.path.join(dir, f\"{sanitized_virus_name}_{sanitized_gene_name}_{gene_id}_datasets\")\n",
    "\n",
    "        # Save the zip file\n",
    "        with open(zip_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Successfully downloaded data for gene {gene_name} from {virus_name}\")\n",
    "\n",
    "        # Extract the zip file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)\n",
    "        print(f\"Extracted {zip_path} to {unzip_path}\")\n",
    "        \n",
    "        # Delete the zip file\n",
    "        os.remove(zip_path)\n",
    "        print(f\"Deleted zip file: {zip_path}\")\n",
    "\n",
    "        # Find the .fna file and extract genomic range\n",
    "        fna_file_path = unzip_path + \"/ncbi_dataset/data/gene.fna\"\n",
    "        file_genomic_range = extract_genomic_range_from_fna(fna_file_path)\n",
    "        genomic_range = file_genomic_range if file_genomic_range else genomic_range  # Use file range if available\n",
    "\n",
    "        # Append metadata to the list\n",
    "        metadata_list.append({\n",
    "            \"Virus_name\": virus_name,\n",
    "            \"Virus_id\": refseq_id,\n",
    "            \"Gene_id\": gene_id,\n",
    "            \"Gene_name\": gene_name,\n",
    "            \"Genomic_range\": genomic_range\n",
    "        })\n",
    "    \n",
    "    else:\n",
    "        print(f\"Failed to download data for gene {gene_name} from {virus_name}: {response.status_code}\")\n",
    "\n",
    "# Loop through virus IDs and download gene files\n",
    "for virus_name, refseq_id in result_dict.items():    \n",
    "    download_dir = os.path.join(META_DATA_DIR + virus_name)  # Directory to save files\n",
    "    os.makedirs(download_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "    gene_info = get_gene_ids_from_refseq(refseq_id)\n",
    "\n",
    "    for gene_id, gene_name, genomic_range in gene_info:\n",
    "        download_gene_data(gene_id, download_dir, gene_name, virus_name, genomic_range)\n",
    "\n",
    "# Convert metadata list to DataFrame and save as CSV\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "metadata_df.to_csv(META_DATA_DIR + \"viral_gene_metadata.csv\", index=False)\n",
    "print(f\"Created metadata file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to make gene fasta files from NCBI viruses without official NCBI ID\n",
    "Copy the full text, of the NCBI entry for the virus you want a fasta file for each gene, to a '<virus_name>_raw.txt' file and put it in a directoroy DATA_DIR/viral_genes/{virus_name}/{virus_name}_raw.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_genes_and_save_fasta(file_path, virus_name, output_dir):\n",
    "        \n",
    "    version = None  # Initialize the version variable\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        all_lines = file.readlines()\n",
    "\n",
    "        lines = iter(all_lines)  # Convert the list to an iterator\n",
    "\n",
    "        # Look for the VERSION line in the file\n",
    "        for line in lines:\n",
    "            if line.startswith(\"VERSION\"):\n",
    "                version = line.split()[1].strip()\n",
    "                break  # Exit after finding the version\n",
    "        \n",
    "        # Initialize sequence string\n",
    "        sequence = \"\"\n",
    "        \n",
    "        # Flag to start reading sequence after ORIGIN\n",
    "        is_origin = False\n",
    "        for line in lines:\n",
    "            if line.startswith(\"ORIGIN\"):\n",
    "                is_origin = True\n",
    "                continue\n",
    "            \n",
    "            # Stop reading when // is encountered\n",
    "            if line.startswith(\"//\"):\n",
    "                break\n",
    "\n",
    "            if is_origin:\n",
    "                # Remove line numbers and spaces, then concatenate the sequence\n",
    "                sequence += ''.join([char.upper() for char in line if char.isalpha()])\n",
    "\n",
    "        lines = iter(all_lines)  # Reset the iterator to the start of the file\n",
    "\n",
    "        # Process each CDS region after finding the version\n",
    "        for line in lines:\n",
    "            if line.startswith(\"     CDS             \"):\n",
    "                # Check if it's a 'join' case or a single range\n",
    "                if \"join(\" in line:\n",
    "                    # Extract ranges within 'join()', e.g., '266..13468,13468..21555'\n",
    "                    ranges = line.split(\"join(\")[1].split(\")\")[0].split(\",\")\n",
    "                    #print(ranges)\n",
    "                    # Convert ranges to start and end pairs\n",
    "                    positions = [(int(r.split(\"..\")[0]), int(r.split(\"..\")[1])) for r in ranges]\n",
    "                elif \"complement(join(\" in line:\n",
    "                    # Extract ranges within 'join()', e.g., '266..13468,13468..21555'\n",
    "                    ranges = line.split(\"complement(join(\")[1].split(\"))\")[0].split(\",\")\n",
    "                    # Convert ranges to start and end pairs\n",
    "                    positions = [(int(r.split(\"..\")[0]), int(r.split(\"..\")[1])) for r in ranges]\n",
    "                elif \"complement(\" in line:\n",
    "                    # Extract ranges within 'join()', e.g., '266..13468,13468..21555'\n",
    "                    ranges = line.split(\"complement(\")[1].split(\")\")[0].split(\",\")\n",
    "                    # Convert ranges to start and end pairs\n",
    "                    positions = [(int(r.split(\"..\")[0]), int(r.split(\"..\")[1])) for r in ranges]\n",
    "                else:\n",
    "                    # Single range case\n",
    "                    start = int(line.split()[1].split(\"..\")[0])\n",
    "                    end = int(line.split()[1].split(\"..\")[1])\n",
    "                    positions = [(start, end)]\n",
    "\n",
    "                # Initialize fields for product and gene_id\n",
    "                product = None\n",
    "                gene_id = \"0000000\"\n",
    "\n",
    "                # Loop to find /product, /protein_id, and /db_xref=\"GeneID\"\n",
    "                while \"/gene=\" not in line and \"/product=\" not in line:                    \n",
    "                    line = next(lines).strip()\n",
    "                if \"/gene=\" in line:\n",
    "                    product = line.split('\"')[1].replace(\"/\", \"-\")\n",
    "                elif \"/product=\" in line:\n",
    "                    product = line.split('\"')[1].replace(\"/\", \"-\")           \n",
    "\n",
    "                while \"/db_xref=\" not in line and \"/protein_id=\" not in line:\n",
    "                    line = next(lines).strip()\n",
    "                if \"/db_xref=\" in line and \"GeneID:\" in line:\n",
    "                    gene_id = line.split(\"GeneID:\")[1].strip('\"')\n",
    "                elif \"/protein_id=\" in line and gene_id == \"0000000\":\n",
    "                    gene_id = line.split('\"')[1]  # Use protein ID if GeneID is missing\n",
    "                \n",
    "\n",
    "                # Create the gene directory and gene.fna file if both positions and product are found\n",
    "                if positions and product:\n",
    "                    # Format the subdirectory name: {virus_name}_{gene_name}_{gene_id}_datasets\n",
    "                    gene_dir = os.path.join(output_dir, f\"{virus_name}_{product}_{gene_id}_datasets\")\n",
    "                    os.makedirs(gene_dir, exist_ok=True)\n",
    "\n",
    "                    # Define the path for the gene.fna file within the subdirectory structure\n",
    "                    gene_data_dir = os.path.join(gene_dir, \"ncbi_dataset/data\")\n",
    "                    os.makedirs(gene_data_dir, exist_ok=True)\n",
    "\n",
    "                    fna_path = os.path.join(gene_data_dir, \"gene.fna\")\n",
    "\n",
    "                    # Extract and concatenate sequences for each position range\n",
    "                    extracted_sequence = \"\".join([sequence[start - 1:end] for start, end in positions])\n",
    "                    \n",
    "                    position_str = \",\".join([f\"{start}-{end}\" for start, end in positions])\n",
    "\n",
    "                    # Write the sequence to the gene.fna file\n",
    "                    with open(fna_path, 'w') as fna_file:\n",
    "                        fna_file.write(f\">{version}:{position_str} {product} [organism={virus_name}]\\n\")\n",
    "                        fna_file.write(f\"{extracted_sequence}\\n\")\n",
    "\n",
    "virus_name = \"BoDV-1\"  # Specify the virus name\n",
    "file_path = META_DATA_DIR + f\"{virus_name}/{virus_name}_raw.txt\"\n",
    "output_dir = META_DATA_DIR + f\"{virus_name}/\"\n",
    "extract_genes_and_save_fasta(file_path, virus_name, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all directories that contain a *_raw.txt file\n",
    "directories_with_raw = [os.path.dirname(f) for f in glob.glob(os.path.join(META_DATA_DIR, \"**/*_raw.txt\"), recursive=True)]\n",
    "\n",
    "for dir_path in directories_with_raw:\n",
    "    # Delete only subdirectories within the directory\n",
    "    for item in os.listdir(dir_path):\n",
    "        item_path = os.path.join(dir_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "    \n",
    "    # Extract virus name based on directory name\n",
    "    virus_name = os.path.basename(dir_path)\n",
    "    file_path = os.path.join(dir_path, f\"{virus_name}_raw.txt\")\n",
    "    output_dir = dir_path\n",
    "    extract_genes_and_save_fasta(file_path, virus_name, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to make gene fasta files from virus sequences form other sources (example: Lentivirus from Addgene website)\n",
    "Copy all fasta sequences you want in one file 'DATA_DIR/viral_genes/{virus_name}/pLenti-raw-genes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_genes_and_save_fasta(file_path, virus_name, output_dir):\n",
    "    gene_name = None\n",
    "    sequence_lines = []\n",
    "    virus_id = None\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            # Detect start of a new gene sequence\n",
    "            if line.startswith(\"id=\"):\n",
    "                virus_buffer = line[4:].strip()  # Update virus_id for each new gene block\n",
    "                if virus_id == None:\n",
    "                    virus_id = virus_buffer\n",
    "            elif line.startswith(\">\"):\n",
    "                # If we have a previous gene loaded, save it to a file\n",
    "                if gene_name:\n",
    "                    save_gene_fasta(gene_name, sequence_lines, virus_name, output_dir, virus_id)\n",
    "                    virus_id = virus_buffer\n",
    "\n",
    "                # Start new gene name and reset sequence\n",
    "                gene_name = line[1:].strip()  # Strip '>' and any whitespace around the gene name\n",
    "                sequence_lines = []  # Reset sequence storage for the new gene\n",
    "            \n",
    "            # Collect sequence lines for the current gene\n",
    "            else:\n",
    "                sequence_lines.append(line)\n",
    "\n",
    "        # Save the last gene in the file\n",
    "        if gene_name:\n",
    "            save_gene_fasta(gene_name, sequence_lines, virus_name, output_dir, virus_id)\n",
    "\n",
    "\n",
    "def save_gene_fasta(gene_name, sequence_lines, virus_name, output_dir, virus_id):\n",
    "    # Format the subdirectory name: {virus_name}_{gene_name}_datasets\n",
    "    gene_dir = os.path.join(output_dir, f\"{virus_name}_{gene_name}_0000000_datasets\")\n",
    "    os.makedirs(gene_dir, exist_ok=True)\n",
    "\n",
    "    # Define the path for the gene.fna file within the subdirectory structure\n",
    "    gene_data_dir = os.path.join(gene_dir, \"ncbi_dataset/data\")\n",
    "    os.makedirs(gene_data_dir, exist_ok=True)\n",
    "    \n",
    "    fna_path = os.path.join(gene_data_dir, \"gene.fna\")\n",
    "    \n",
    "    # Combine all sequence lines into a single string without newlines in between\n",
    "    full_sequence = ''.join(sequence_lines)\n",
    "    \n",
    "    # Write the sequence to the gene.fna file\n",
    "    with open(fna_path, 'w') as fna_file:\n",
    "        fna_file.write(f\">{virus_id}:0-0 {gene_name} [organism={virus_name}]\\n\")\n",
    "        fna_file.write(f\"{full_sequence}\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "virus_name = \"pLenti\" \n",
    "file_path = META_DATA_DIR + f\"{virus_name}/pLenti-raw-genes.txt\"\n",
    "output_dir = META_DATA_DIR + f\"{virus_name}/\"\n",
    "extract_genes_and_save_fasta(file_path, virus_name, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to make the metadata file with all the viruses\n",
    "Is needed if viruses were added manually (with code above) and not downloaded with API code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base directory where the viral gene directories are stored\n",
    "base_dir = DATA_DIR + \"viral_genes\"\n",
    "\n",
    "# Regular expression to match the directory name pattern\n",
    "# This regex will capture:\n",
    "# - virus_name as the part before the first underscore\n",
    "# - gene_name as the part after the first underscore up to the gene_id\n",
    "# - gene_id as the part before \"_datasets\" which can include letters, numbers, dots, or parentheses\n",
    "pattern = r\"^([^_]+)_(.+)_(\\w+(\\.\\w+)?\\(?\\d*\\)?)_datasets$\"\n",
    "\n",
    "# Initialize an empty list to store our data\n",
    "data = []\n",
    "\n",
    "# Walk through each virus directory in the base directory\n",
    "for virus_dir in os.listdir(base_dir):\n",
    "    virus_path = os.path.join(base_dir, virus_dir)\n",
    "    \n",
    "    # Ensure we're working with directories\n",
    "    if os.path.isdir(virus_path):\n",
    "        \n",
    "        # Now, look for each gene subdirectory\n",
    "        for gene_dir in os.listdir(virus_path):\n",
    "            gene_path = os.path.join(virus_path, gene_dir)\n",
    "            \n",
    "            # Only proceed if it's indeed a directory\n",
    "            if os.path.isdir(gene_path):\n",
    "                \n",
    "                # Match the directory name to extract virus_name, gene_name, and gene_id\n",
    "                match = re.match(pattern, gene_dir)\n",
    "                if not match:\n",
    "                    print(f\"Skipping directory {gene_dir} as it does not match the expected pattern.\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the parts from the regex match\n",
    "                virus_name, gene_name, gene_id = match.groups()[:3]\n",
    "\n",
    "                # Path to the gene.fna file\n",
    "                fna_file_path = os.path.join(gene_path, \"ncbi_dataset\", \"data\", \"gene.fna\")\n",
    "                \n",
    "                # Check if the gene.fna file exists\n",
    "                if os.path.isfile(fna_file_path):\n",
    "                    # Read the first line of the gene.fna file\n",
    "                    with open(fna_file_path, 'r') as fna_file:\n",
    "                        first_line = fna_file.readline().strip()\n",
    "                        \n",
    "                        # Extract virus_id and genomic_range\n",
    "                        try:\n",
    "                            header_parts = first_line.split(\" \")\n",
    "                            virus_id, genomic_range = header_parts[0][1:].split(\":\")\n",
    "                        except ValueError:\n",
    "                            print(f\"Skipping file {fna_file_path} due to unexpected header format.\")\n",
    "                            continue\n",
    "\n",
    "                        # Append the extracted data to our list\n",
    "                        data.append({\n",
    "                            \"Virus_name\": virus_name,\n",
    "                            \"Virus_id\": virus_id,\n",
    "                            \"Gene_id\": gene_id,\n",
    "                            \"Gene_name\": gene_name,\n",
    "                            \"Genomic_range\": genomic_range\n",
    "                        })\n",
    "\n",
    "# Create a DataFrame from the data and save it to a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(f\"{base_dir}/viral_gene_metadata.csv\", index=False)\n",
    "print(\"Data has been written to viral_gene_metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to create a reference gene fasta file out of the human genome and different viral genes\n",
    "Needed if you want to run the Salmon quantification without decoy, otherwise not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gz_fasta_file = Path(HUMAN_GENE_DIR + 'gencode.v38.transcripts.fa.gz') # Path to the compressed human genome .gz FASTA file\n",
    "viral_dir = META_DATA_DIR # Set the directory path where the .fa and .fna files are located\n",
    "output_file = DATA_DIR + \"reference_genes.fasta\" # output file\n",
    "\n",
    "# Search for all .fa and .fna files in the directory\n",
    "# fasta_files = [gz_fasta_file] + list(viral_dir.glob(\"HPV16/**/gene.fna\")) + list(viral_dir.glob(\"HPV18/**/gene.fna\"))\n",
    "fasta_files = [gz_fasta_file] + list(viral_dir.glob(\"**/**/gene.fna\"))\n",
    "\n",
    "# Convert the Path objects to strings\n",
    "fasta_files = [str(f) for f in fasta_files]\n",
    "print(f\"Found {len(fasta_files)} files: {fasta_files}\")\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, \"w\") as outfile:\n",
    "    for fasta_file in fasta_files:\n",
    "        if fasta_file.endswith(\".gz\"):\n",
    "            # Handle compressed files\n",
    "            with gzip.open(fasta_file, \"rt\") as handle:\n",
    "                for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                    SeqIO.write(record, outfile, \"fasta\")\n",
    "        else:\n",
    "            # Handle uncompressed files\n",
    "            with open(fasta_file, \"r\") as handle:\n",
    "                for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                    SeqIO.write(record, outfile, \"fasta\")\n",
    "\n",
    "print(f\"Reference FASTA file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to create a gentrome file (human + all virus)\n",
    "Needed if you want to run the Salmon quantification with decoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this terminal line is way faster, though\n",
    "# gunzip -c human_genom/gencode.v38.transcripts.fa.gz > gentrome.fa # Unzip the transcripts file and create gentrome.fa\n",
    "# cat viral_genes/**/**/ncbi_dataset/data/gene.fna >> gentrome.fa # Concatenate viral gene files into gentrome.fa\n",
    "# gunzip -c human_genom/GRCh38.primary_assembly.genome.fa.gz >> gentrome.fa # Unzip the genome file and append it to gentrome.fa\n",
    "# # gzip gentrome.fa # Compress the final output file\n",
    "\n",
    "# Paths to files\n",
    "transcriptome_file = Path(HUMAN_GENE_DIR + 'gencode.v38.transcripts.fa.gz')\n",
    "genome_file = Path(HUMAN_GENE_DIR + 'GRCh38.primary_assembly.genome.fa.gz')\n",
    "viral_dir = META_DATA_DIR  # Directory containing viral gene .fa files\n",
    "output_file = Path(DATA_DIR + 'gentrome.fa.gz')\n",
    "\n",
    "# Open the output file in gzipped mode\n",
    "with gzip.open(output_file, \"wt\") as outfile:\n",
    "    # Write human transcriptome\n",
    "    with gzip.open(transcriptome_file, \"rt\") as t_file:\n",
    "        for record in SeqIO.parse(t_file, \"fasta\"):\n",
    "            SeqIO.write(record, outfile, \"fasta\")    \n",
    "    \n",
    "    # Write viral gene files\n",
    "    for viral_file in viral_dir.glob(\"**/**/gene.fna\"):\n",
    "        with open(viral_file, \"r\") as v_file:\n",
    "            for record in SeqIO.parse(v_file, \"fasta\"):\n",
    "                SeqIO.write(record, outfile, \"fasta\")\n",
    "    \n",
    "    # Write human genome\n",
    "    with gzip.open(genome_file, \"rt\") as g_file:\n",
    "        for record in SeqIO.parse(g_file, \"fasta\"):\n",
    "            SeqIO.write(record, outfile, \"fasta\")\n",
    "\n",
    "print(f\"gentrome.fa.gz created at: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to create a decoy list out of human genes\n",
    "Needed if you want to run the Salmon quantification with decoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster to use this terminal lines\n",
    "# grep \"^>\" <(gunzip -c GRCh38.primary_assembly.genome.fa.gz) | cut -d \" \" -f 1 > decoys.txt\n",
    "# sed -i.bak -e 's/>//g' decoys.txt\n",
    "\n",
    "# Create decoy list\n",
    "human_genome_file = Path(HUMAN_GENE_DIR + 'GRCh38.primary_assembly.genome.fa.gz')\n",
    "decoy_output = DATA_DIR + \"decoys.txt\"\n",
    "\n",
    "# Open output decoy file\n",
    "with open(decoy_output, \"w\") as decoy_file:\n",
    "    # Write human genome decoys\n",
    "    for line in gzip.open(human_genome_file, 'rt'):\n",
    "        if line.startswith(\">\"):\n",
    "            decoy_file.write(line[1:].split(\" \")[0] + \"\\n\")\n",
    "\n",
    "print(f\"Decoy list created: {decoy_output}\")\n",
    "\n",
    "# To create the index with the decoy, use this Salmon command:\n",
    "# salmon index -t gentrome.fa.gz -d decoys.txt -i salmon_index --gencode -k 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to check which cell lines were not aligned yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bucket path and output CSV file\n",
    "bucket_path = \"gs://virus_expression_results/transcript_quant_results/\"\n",
    "output_file = SCRIPTS_DIR + \"missing_cell_lines.csv\"\n",
    "\n",
    "# Run a faster gsutil command without '-d' and process results to get directories only\n",
    "command = [\"gsutil\", \"ls\", f\"{bucket_path}\"]\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Extract directories by filtering out non-directory paths\n",
    "directories = [line for line in result.stdout.splitlines() if line.endswith('/')]\n",
    "directory_names = [line.replace(bucket_path, \"\").strip(\"/\") for line in directories]\n",
    "\n",
    "# Load the CSV file with all cell lines\n",
    "all_cell_lines = pd.read_csv(f\"{SCRIPTS_DIR}cell_line_google_bucket_index.csv\")\n",
    "\n",
    "# Filter rows where 'ModelID' is not in directory_names\n",
    "missing_cell_lines = all_cell_lines[~all_cell_lines['entity:sample_id'].isin(directory_names)]\n",
    "\n",
    "# Check if there are any missing cell lines and save or print message\n",
    "if missing_cell_lines.empty:\n",
    "    print(\"All cell lines were already aligned\")\n",
    "else:\n",
    "    missing_cell_lines.to_csv(output_file, index=False)\n",
    "    print(f\"Missing cell lines saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to map the ModelId to the Sequencing Id\n",
    "If you need to map certain ModelIDs (ACH-00xxxx) to Sequencing IDs (CDS-xxxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = pd.read_csv(f\"{GENERAL_DATA}GumboQueryExport.csv\")\n",
    "cell_lines_df = pd.read_csv(f\"{PLOT_DIR}/priority_cell_lines.csv\")\n",
    "\n",
    "# Create a dictionary for mapping column names to IDs\n",
    "name_to_id = dict(zip(id_map['model_id'], id_map['sequencing_id']))\n",
    "cell_lines_df['CDS_ID'] = cell_lines_df['ModelID'].map(name_to_id)\n",
    "cell_lines_df.to_csv(f\"{PLOT_DIR}/priority_cell_lines_mapped.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to write column with ModelID and bam file location and filter out already calculated cell lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam_files_df = pd.read_csv(f\"{SCRIPTS_DIR}bam_file_names.csv\")\n",
    "id_map = pd.read_csv(f\"{GENERAL_DATA}GumboQueryExport.csv\")\n",
    "all_viral_genes_heatmap_df = pd.read_csv(f'{PLOT_DIR}all_viral_genes_heatmap_data.csv', index_col=0)\n",
    "\n",
    "# Create a dictionary for mapping column names to IDs\n",
    "name_to_id = dict(zip(id_map['sequencing_id'], id_map['model_id']))\n",
    "\n",
    "# Get sequencing id from bucket path and map to ModelID, remove NaNs and duplicates\n",
    "modelid_bucket_df = pd.DataFrame(columns= [\"entity:sample_id\", \"hg38_rna_bam\", \"sequencing_id\"])\n",
    "modelid_bucket_df[\"hg38_rna_bam\"] = bam_files_df[\"hg38_rna_bam\"]\n",
    "modelid_bucket_df['sequencing_id'] = modelid_bucket_df['hg38_rna_bam'].str.extract(r'(CDS-\\w{6})')\n",
    "modelid_bucket_df[\"entity:sample_id\"] = modelid_bucket_df['sequencing_id'].map(name_to_id)\n",
    "modelid_bucket_df = modelid_bucket_df.dropna(subset=['entity:sample_id']).drop_duplicates(subset='entity:sample_id')\n",
    "\n",
    "# Filter out already calculated cell lines\n",
    "modelid_bucket_df_filtered = modelid_bucket_df[~modelid_bucket_df['entity:sample_id'].isin(all_viral_genes_heatmap_df.index)]\n",
    "modelid_bucket_df_sorted= modelid_bucket_df_filtered.sort_values(by='entity:sample_id').iloc[:,:2]\n",
    "modelid_bucket_df_sorted.to_csv(f\"{SCRIPTS_DIR}cell_line_google_bucket_index2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to prepare the hpv and cellosaurus data and map the given IDs to the DepmapIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpv_cell_data = pd.read_csv(f\"{DATA_DIR}HPV.csv\")\n",
    "id_map = pd.read_csv(f\"{DATA_DIR}GumboQueryExport.csv\")\n",
    "cellosaurus = pd.read_csv(f\"{DATA_DIR}Cellosaurus HPV cell lines.xlsx - Sheet1.csv\")\n",
    "id_map_ = pd.read_csv(f\"{DATA_DIR}internal-24q2_v87-model.csv\")\n",
    "\n",
    "# Create a dictionary for mapping column names to IDs\n",
    "name_to_id = dict(zip(id_map['sequencing_id'], id_map['model_id']))\n",
    "\n",
    "# Rename columns of df1 using the mapping dictionary\n",
    "hpv_cell_data = hpv_cell_data.rename(columns=name_to_id)\n",
    "hpv_cell_data.to_csv(f\"{DATA_DIR}HPV_mapped.csv\")\n",
    "\n",
    "# Create a dictionary for mapping column names to IDs\n",
    "name_to_id_ = dict(zip(id_map_['RRID'], id_map_['ModelID']))\n",
    "\n",
    "cellosaurus.columns = ['RRID', 'ModelID', '', '','Cancer','HPV','Type','','']\n",
    "cellosaurus['ModelID'] = cellosaurus['RRID'].map(name_to_id_)\n",
    "cellosaurus = cellosaurus.sort_values(by='ModelID')\n",
    "cellosaurus.to_csv(f\"{DATA_DIR}cellosaurus_mapped.csv\")\n",
    "\n",
    "hpv_cell_data_transposed = hpv_cell_data.T\n",
    "hpv_cell_data_transposed.columns = hpv_cell_data_transposed.iloc[0] # Set the first row (which contains target IDs) as the new column names\n",
    "hpv_cell_data_transposed = hpv_cell_data_transposed[1:] # Drop the first row which is now the header\n",
    "hpv_cell_data_transposed.index.name = 'cell line' # Rename the index to something meaningful\n",
    "hpv_cell_data_transposed = hpv_cell_data_transposed.sort_values(by='cell line')\n",
    "hpv_cell_data_transposed.to_csv(f\"{DATA_DIR}HPV_mapped_transposed.csv\")\n",
    "\n",
    "\n",
    "# Filter cell lines that have HPV genes in them\n",
    "hpv_cell_data_filtered = hpv_cell_data.drop(columns=hpv_cell_data.columns[(hpv_cell_data == 0).all()])\n",
    "hpv_cell_data_filtered.to_csv(f\"{DATA_DIR}HPV_mapped_filtered.csv\")\n",
    "\n",
    "hpv_cell_data_filtered_transpose = hpv_cell_data_filtered.T\n",
    "hpv_cell_data_filtered_transpose.columns = hpv_cell_data_filtered_transpose.iloc[0] # Set the first row (which contains target IDs) as the new column names\n",
    "hpv_cell_data_filtered_transpose = hpv_cell_data_filtered_transpose[1:] # Drop the first row which is now the header\n",
    "hpv_cell_data_filtered_transpose.index.name = 'cell line' # Rename the index to something meaningful\n",
    "hpv_cell_data_filtered_transpose = hpv_cell_data_filtered_transpose.sort_values(by='cell line')\n",
    "hpv_cell_data_filtered_transpose.to_csv(f\"{DATA_DIR}HPV_mapped_filtered_transposed.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
